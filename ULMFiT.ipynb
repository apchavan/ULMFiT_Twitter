{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ULMFiT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uqA47ebBJoa",
        "colab_type": "text"
      },
      "source": [
        "## ULMFiT model for Twitter US Airlines Sentiment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfPmO8SLAVco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary libraries.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sMXRTD_BEZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We disable warnings & only show errors of TensorFlow.\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj_OSDZbGdLA",
        "colab_type": "text"
      },
      "source": [
        "Now read **Twitter.csv** file as a Pandas dataframe, randomize data & see first 10 records of features, targets from dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_ahCPtqBHvY",
        "colab_type": "code",
        "outputId": "81ee7867-4b31-497c-be16-725de3d07dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Read data in CSV file as a Pandas DataFrame.\n",
        "twitter_df = pd.read_csv('Tweets.csv', sep=',')\n",
        "\n",
        "# Now randomize data using permutation to avoid trouble if the data is in some sorted order.\n",
        "twitter_df = twitter_df.reindex(np.random.permutation(twitter_df.index))\n",
        "\n",
        "# Take a look at our features (i.e. 'text' & 'airline') & target (i.e. 'airline_sentiment') of dataframe.\n",
        "twitter_df[['text', 'airline', 'airline_sentiment']].head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8817</th>\n",
              "      <td>@JetBlue @ProfessorpaUL15 Always happy to help!</td>\n",
              "      <td>Delta</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14299</th>\n",
              "      <td>As am I, @AmericanAir - but thankfully there w...</td>\n",
              "      <td>American</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8039</th>\n",
              "      <td>@JetBlue Now ur asking for the heavy guns! You...</td>\n",
              "      <td>Delta</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2856</th>\n",
              "      <td>@united thanks for having ground crews that ar...</td>\n",
              "      <td>United</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>821</th>\n",
              "      <td>@united yes in Lusaka, Zambia. My guess is the...</td>\n",
              "      <td>United</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9582</th>\n",
              "      <td>@USAirways will all flights out of DFW be Canc...</td>\n",
              "      <td>US Airways</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5150</th>\n",
              "      <td>@SouthwestAir baggage claim has already change...</td>\n",
              "      <td>Southwest</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7569</th>\n",
              "      <td>@JetBlue after applying my new tag, she left m...</td>\n",
              "      <td>Delta</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14636</th>\n",
              "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
              "      <td>American</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3157</th>\n",
              "      <td>@united Surprised to go from 1K last year to n...</td>\n",
              "      <td>United</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  ... airline_sentiment\n",
              "8817     @JetBlue @ProfessorpaUL15 Always happy to help!  ...          positive\n",
              "14299  As am I, @AmericanAir - but thankfully there w...  ...          negative\n",
              "8039   @JetBlue Now ur asking for the heavy guns! You...  ...           neutral\n",
              "2856   @united thanks for having ground crews that ar...  ...          negative\n",
              "821    @united yes in Lusaka, Zambia. My guess is the...  ...          negative\n",
              "9582   @USAirways will all flights out of DFW be Canc...  ...           neutral\n",
              "5150   @SouthwestAir baggage claim has already change...  ...          negative\n",
              "7569   @JetBlue after applying my new tag, she left m...  ...          negative\n",
              "14636  @AmericanAir leaving over 20 minutes Late Flig...  ...          negative\n",
              "3157   @united Surprised to go from 1K last year to n...  ...           neutral\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRXZEYANHyad",
        "colab_type": "text"
      },
      "source": [
        "Here, in feature column '**text**', we have some non-ASCII characters (e.g. emoji) like ™, œ. So we clean-up our text data using following function: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqy3IC_OEf4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_non_ascii(text):\n",
        "    \"\"\"\n",
        "    Function to remove all non-ASCII characters (e.g. emoji) from given text.\n",
        "    :param text: Text of review from user in data.\n",
        "    :return: Text string with removed all non-ASCII characters from it.\n",
        "    \"\"\"\n",
        "    return ''.join(i for i in text if ord(i) < 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNhRd2L4E-bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's apply this function to our 'text' feature column in dataframe.\n",
        "twitter_df['text'] = twitter_df['text'].apply(func=clean_non_ascii)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-QI_D99Ja05",
        "colab_type": "text"
      },
      "source": [
        "We create four dataframes respectively for: \n",
        "**Train features**, **Train targets**, **Test features**, **Test targets**. <br />\n",
        "That's because to separate training & testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHf8CRlMJbRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = pd.DataFrame()\n",
        "train_targets = pd.DataFrame()\n",
        "test_features = pd.DataFrame()\n",
        "test_targets = pd.DataFrame()\n",
        "\n",
        "# Take first 10000 records for training features & targets respectively.\n",
        "train_features['text'] = twitter_df['text'].head(n=10000)\n",
        "train_features['airline'] = twitter_df['airline'].head(n=10000)\n",
        "train_targets['airline_sentiment'] = twitter_df['airline_sentiment'].head(n=10000)\n",
        "\n",
        "# Similarly features & targets above 10000 records will be for test.\n",
        "test_features['text'] = twitter_df['text'].iloc[len(train_features.index):]\n",
        "test_features['airline'] = twitter_df['airline'].iloc[len(train_features.index):]\n",
        "test_targets['airline_sentiment'] = twitter_df['airline_sentiment'].iloc[len(train_targets.index):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiCBaqxHKlPx",
        "colab_type": "text"
      },
      "source": [
        "Now inspect how many **airline** companies & unique classes **airline_sentiment** are present using `describe()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_t2Y8WSLMeO",
        "colab_type": "code",
        "outputId": "580991a0-0281-4bf1-f6e3-dceb85b0063b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "print(\" {*} Details of 'airline' feature column: \\n\", twitter_df['airline'].describe(), \\\n",
        "      \"\\n\\n {*} Details of 'airline_sentiment' target column: \\n\", twitter_df['airline_sentiment'].describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " {*} Details of 'airline' feature column: \n",
            " count      14640\n",
            "unique         6\n",
            "top       United\n",
            "freq        3822\n",
            "Name: airline, dtype: object \n",
            "\n",
            " {*} Details of 'airline_sentiment' target column: \n",
            " count        14640\n",
            "unique           3\n",
            "top       negative\n",
            "freq          9178\n",
            "Name: airline_sentiment, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0--91LvMUfC",
        "colab_type": "text"
      },
      "source": [
        "So here '**`unique`**' section of both results shows the actual unique records in those dataframe columns. It means **6** companies of '**airline**' (feature) & **3** classes of '**airline_sentiment**' (target)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_iEHE3XN3Xh",
        "colab_type": "text"
      },
      "source": [
        "We have '**airline**' (feature) & '**airline_sentiment**' (target) in text form. So let's use ***One-hot encoding*** to convert string format to processable numeric format. <br/> We first convert string to integer and then integer to one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZjau_TPU7C",
        "colab_type": "code",
        "outputId": "0d42788b-56fd-4a7e-ca76-20c04b083212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "# First use 'LabelEncoder()' in Scikit-learn to convert from string to integer then use 'to_categorical()' of Keras to get respective one-hot encoding.\n",
        "# One-hot encoding for training set 'airline' feature:\n",
        "label_encoder_train_airline = LabelEncoder()\n",
        "train_airline_feature_int = label_encoder_train_airline.fit_transform(y=train_features['airline'])\n",
        "train_airline_feature_encoded = tf.keras.utils.to_categorical(y=train_airline_feature_int, \n",
        "                                                              num_classes=train_features['airline'].describe()['unique'])\n",
        "\n",
        "# One-hot encoding for test set 'airline' feature:\n",
        "label_encoder_test_airline = LabelEncoder()\n",
        "test_airline_feature_int = label_encoder_test_airline.fit_transform(y=test_features['airline'])\n",
        "test_airline_feature_encoded = tf.keras.utils.to_categorical(y=test_airline_feature_int, \n",
        "                                                             num_classes=test_features['airline'].describe()['unique'])\n",
        "\n",
        "# Conversion for training targets 'airline_sentiment':\n",
        "label_encoder_train_targets = LabelEncoder()\n",
        "train_targets_int = label_encoder_train_targets.fit_transform(y=train_targets['airline_sentiment'])\n",
        "train_targets_encoded = tf.keras.utils.to_categorical(y=train_targets_int, \n",
        "                                                      num_classes=train_targets['airline_sentiment'].describe()['unique'])\n",
        "\n",
        "# Similarly conversion for test targets 'airline_sentiment':\n",
        "label_encoder_test_targets = LabelEncoder()\n",
        "test_targets_int = label_encoder_test_targets.fit_transform(y=test_targets['airline_sentiment'])\n",
        "test_targets_encoded = tf.keras.utils.to_categorical(y=test_targets_int, \n",
        "                                                     num_classes=test_targets['airline_sentiment'].describe()['unique'])\n",
        "\n",
        "# Let's see how encoding is done.\n",
        "print(' {*} Oirginal train targets: \\n', train_targets['airline_sentiment'].head(5), \\\n",
        "     '\\n\\n {*} One-hot encoded train targets: \\n', [train_targets_encoded[i] for i in range(5)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " {*} Oirginal train targets: \n",
            " 8817     positive\n",
            "14299    negative\n",
            "8039      neutral\n",
            "2856     negative\n",
            "821      negative\n",
            "Name: airline_sentiment, dtype: object \n",
            "\n",
            " {*} One-hot encoded train targets: \n",
            " [array([0., 0., 1.], dtype=float32), array([1., 0., 0.], dtype=float32), array([0., 1., 0.], dtype=float32), array([1., 0., 0.], dtype=float32), array([1., 0., 0.], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-YpRie-RjiS",
        "colab_type": "text"
      },
      "source": [
        "The feature column '**text**' has textual setences so we use `Tokenizer` that vectorize collection of written text into sequence of integers. That means we use **word embedding**. <br />First we create an object of `Tokenizer` & define a function that fits tokenizer on given text and then return the sequence of integers for that text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TibHElvzRfz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Tokenizer object.\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "def get_tokenizer_encoding(text=\"\"):\n",
        "  \"\"\"\n",
        "  Function to get sequence of integers for given text.\n",
        "  :param text: Text of review from user in data.\n",
        "  :return: Sequence or vector of integers for given text.\n",
        "  \"\"\"\n",
        "  tokenizer.fit_on_texts(texts=text)\n",
        "  return tokenizer.texts_to_sequences(texts=text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfgMkmpjUlVt",
        "colab_type": "code",
        "outputId": "b10002f3-9801-4286-d87c-c3b3c488eac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Transform and get tokenize encoded text features.\n",
        "train_text_feature_tokenize_encoded = get_tokenizer_encoding(text=train_features['text'])\n",
        "test_text_feature_tokenize_encoded = get_tokenizer_encoding(text=test_features['text'])\n",
        "\n",
        "# Let's check how the sequence is encoded for first sentence in text data.\n",
        "print(' Original sentence: ', train_features['text'][0], '\\n Encoded sequence: ', train_features_tokenize_encoded[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original sentence:  @VirginAmerica What @dhepburn said. \n",
            " Encoded sequence:  [12, 71, 115, 3, 11524, 1501, 94, 82, 172, 2504]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtolpeDeWgtq",
        "colab_type": "text"
      },
      "source": [
        "We have to get maximum sequence length of encoded text between train & test data, to make all sequences of equal length.\n",
        "<br/>Then use `pad_sequences()` of Keras in order to get every encoded text sequence having the same length. Since we can't use sequences of different lengths for processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbb4cqLaVmbV",
        "colab_type": "code",
        "outputId": "9b4f49b6-2ad8-4146-d0a7-ca5a7e846f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Initially get maximum length of encoded text between train & test encoded data.\n",
        "max_length_text = len(max(train_features_tokenize_encoded, key=len)) \\\n",
        "    if len(max(train_features_tokenize_encoded, key=len)) > len(max(test_features_tokenize_encoded, key=len)) \\\n",
        "    else len(max(test_features_tokenize_encoded, key=len))\n",
        "print(' Maximum length of encoded text sequence: ', max_length_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Maximum length of encoded text sequence:  35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyGQnCnNY2pd",
        "colab_type": "code",
        "outputId": "7e363bad-11ce-4d64-e00b-40c93bcffd89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# Get equal sequence lengths using 'pad_sequences()' for train & test sets.\n",
        "train_text_feature_padded = tf.keras.preprocessing.sequence.pad_sequences(sequences=train_text_feature_tokenize_encoded,\n",
        "                                                                          truncating='pre', padding='post',\n",
        "                                                                          maxlen=max_length_text)\n",
        "\n",
        "test_text_feature_padded = tf.keras.preprocessing.sequence.pad_sequences(sequences=test_text_feature_tokenize_encoded,\n",
        "                                                                         truncating='pre', padding='post',\n",
        "                                                                         maxlen=max_length_text)\n",
        "print(' {*} Padded sequences with equal shapes ', train_features_padded[0].shape, ' each (training features): \\n')\n",
        "print(train_features_padded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " {*} Padded sequences with equal shapes  (35,)  each (training features): \n",
            "\n",
            "[[ 12  71 115 ...   0   0   0]\n",
            " [ 16 234   9 ...   0   0   0]\n",
            " [ 13  93  40 ...   0   0   0]\n",
            " ...\n",
            " [ 16 320 688 ...   0   0   0]\n",
            " [ 13  38  75 ...   0   0   0]\n",
            " [ 13  57 140 ...   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwXBdWEexs-f",
        "colab_type": "text"
      },
      "source": [
        "Now we have to get total vocabulary size which is necessary for embedding (first) layer in the analysis model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yQmdhSscLhY",
        "colab_type": "code",
        "outputId": "db825750-97b7-4536-b562-a8f670ee4ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get total vocabulary size using 'word_index' property of tokenizer object which is required for embedding layer in our model.\n",
        "vocabulary_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index.\n",
        "print(' The vocabulary size for our data is: ', vocabulary_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The vocabulary size for our data is:  15769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJPyvP26r9OL",
        "colab_type": "text"
      },
      "source": [
        "Next we'll define our model to train & test on data. <br />\n",
        "Model architecture: <br />\n",
        "**1. Embedding layer** -> This is the first layer in our model since we have text as word embeddings in feature. So it is important to learn those embeddings. <br />\n",
        "**2. LSTM** -> Long Short-Term Memory are hidden layers that can maintain their state, which allows to handle sentences where next word depends on previous. Our model's feautre is textual sentences (with word embeddings) so we provided two LSTMs with 100 memory cells each since more memory cells & deeper network may get better results. <br />\n",
        "**3. Dense** -> Dense is fully connected layer which connects 100 neurons of LSTM in hidden layer to interpret the features extracted from sequence. At the end, another Dense layer with 'softmax' activation is used with 3 neurons to get result among the possible three classes of target (i.e. negative, neutral or positive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaLFYRWtcRRy",
        "colab_type": "code",
        "outputId": "5282119c-9125-43e1-91a6-88c12a19671b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "# Define model.\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=vocabulary_size, input_length=max_length_text,\n",
        "                                    output_dim=50))\n",
        "model.add(tf.keras.layers.LSTM(units=100, return_sequences=True))\n",
        "model.add(tf.keras.layers.LSTM(units=100))\n",
        "model.add(tf.keras.layers.Dense(units=100, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
        "\n",
        "print(' {*} Model architecture: \\n')\n",
        "print(model.summary())\n",
        "\n",
        "# Compile model.\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " {*} Model architecture: \n",
            "\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 35, 50)            796700    \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 35, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 947,903\n",
            "Trainable params: 947,903\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zi_-0jDcV7p",
        "colab_type": "code",
        "outputId": "1cef0a55-c385-4e7b-c3c8-c8517463ccaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "print('\\n\\n Training model... \\n')\n",
        "model.fit(x=[train_text_feature_padded, train_airline_feature_encoded], y=train_targets_encoded,\n",
        "          batch_size=64, epochs=10, steps_per_epoch=100)\n",
        "print('\\n\\n Training finished... \\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Training model... \n",
            "\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1077s 11s/step - loss: 0.0476 - acc: 0.9832\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1073s 11s/step - loss: 0.0447 - acc: 0.9880\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1075s 11s/step - loss: 0.0161 - acc: 0.9962\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1075s 11s/step - loss: 0.0123 - acc: 0.9966\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1076s 11s/step - loss: 0.0110 - acc: 0.9965\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1075s 11s/step - loss: 0.0097 - acc: 0.9966\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1077s 11s/step - loss: 0.0798 - acc: 0.9736\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1093s 11s/step - loss: 0.0664 - acc: 0.9787\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1098s 11s/step - loss: 0.0218 - acc: 0.9925\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1096s 11s/step - loss: 0.0153 - acc: 0.9954\n",
            "\n",
            "\n",
            " Training finished... \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8IiSB5z1wlk",
        "colab_type": "text"
      },
      "source": [
        "After training, we'll evaluate model on test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcVewwkL6Syb",
        "colab_type": "code",
        "outputId": "512b749c-af5d-4276-8575-838f6db64539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Evaluate model on test data.\n",
        "loss_acc = model.evaluate(x=[test_text_feature_padded, test_airline_feature_encoded], y=test_targets_encoded, verbose=1)\n",
        "print(' Test data:= Loss: %0.6f, Accuracy: %0.2f%%' % (loss_acc[0], loss_acc[1] * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4640/4640 [==============================] - 3s 702us/sample - loss: 1.0975 - acc: 0.6265\n",
            " Test data:= Loss: 1.097461, Accuracy: 62.65%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr0TfYOyXHZy",
        "colab_type": "text"
      },
      "source": [
        "Our model can correctly classify  more than half of given data (62.65%) on training with just 10000 records. <br />\n",
        "This can be improved significantly by providing more training data to the model, which will increase its vocabulary size also. Another way may be using set of pre-built embeddings such as **GloVe (\"global vectors for word representation\")**, which is constructed using the text of Wikipedia. <br />\n",
        "So here, by creating embeddings on the fly our model performed good on test data with limited set of training."
      ]
    }
  ]
}